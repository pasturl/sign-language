{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ba4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436d594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../trained_models/signs_model_efficientnetv2-s_v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5f7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lite_model_path = \"../trained_models/signs_model_efficientnetv2-s_v0.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6826c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names =[\"Sweet milk\", \"Argentina\", \"Barbecue\", \"Thanks\"]\n",
    "frames_to_rolling_mean = 10\n",
    "predictions = []\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "bottomLeftCornerOfText = (10,500)\n",
    "fontScale = 1\n",
    "fontColor = (255,255,255)\n",
    "thickness = 1\n",
    "lineType = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69409388",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(lite_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c87b4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "978dbc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15329695  1.0730786   0.40879187 -1.4307716 ]]\n"
     ]
    }
   ],
   "source": [
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3395e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(lite_model_path):\n",
    "    interpreter = tf.lite.Interpreter(lite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    return interpreter\n",
    "\n",
    "\n",
    "model = load_model(lite_model_path)\n",
    "# Get input and output tensors.\n",
    "input_details = model.get_input_details()\n",
    "output_details = model.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "\n",
    "\n",
    "width = input_shape[1]\n",
    "height = input_shape[2]\n",
    "dim = (width, height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b56682e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84f2eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d062f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.expand_dims(np.array(resized, dtype=np.float32), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be9221d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_tensor(input_details[0]['index'], input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ab25805",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7ca42a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "prediction_scores = model.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49cccffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0475292,  0.5876383, -0.7153858, -0.6642868]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "326ca207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_scores = model.predict(np.expand_dims(resized, axis=0))\n",
    "predicted_index = np.argmax(prediction_scores)\n",
    "class_predicted = class_names[predicted_index]\n",
    "predictions.append(prediction_scores)\n",
    "rolling_mean_prediction = np.mean(predictions[-frames_to_rolling_mean:], axis=0)\n",
    "predicted_index = np.argmax(rolling_mean_prediction)\n",
    "class_predicted = class_names[predicted_index]\n",
    "max_pred = rolling_mean_prediction[0][predicted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a85c3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0475292,  0.5876383, -0.7153858, -0.6642868]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78f3752f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "846"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_details[0]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c929e1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0475292"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee5eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # The function `get_tensor()` returns a copy of the tensor data.\n",
    "        # Use `tensor()` in order to get a pointer to the tensor.\n",
    "        prediction_scores = model.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        #prediction_scores = model.predict(np.expand_dims(resized, axis=0))\n",
    "        predicted_index = np.argmax(prediction_scores)\n",
    "        class_predicted = class_names[predicted_index]\n",
    "        predictions.append(prediction_scores)\n",
    "        rolling_mean_prediction = np.mean(predictions[-frames_to_rolling_mean:], axis=0)\n",
    "        predicted_index = np.argmax(rolling_mean_prediction)\n",
    "        class_predicted = class_names[predicted_index]\n",
    "        max_pred = rolling_mean_prediction[0][predicted_index]\n",
    "        if max_pred>0.1:\n",
    "            cv2.putText(frame, class_predicted, (10, 50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1, (0, 0, 255), 2, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4547f655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_input_1:0',\n",
       "  'index': 0,\n",
       "  'shape': array([  1, 384, 384,   3]),\n",
       "  'shape_signature': array([ -1, 384, 384,   3]),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "192d3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_signature = interpreter.get_signature_runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7324271",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_input_data = interpreter.get_input_details()[0][\"shape\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d24065af",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.zeros(dim_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f32695fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SignatureRunner' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmy_signature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SignatureRunner' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "my_signature.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ded4ef4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid Input name (x) for SignatureDef",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmy_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_input_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_input_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\signem\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:246\u001b[0m, in \u001b[0;36mSignatureRunner.__call__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_name, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    245\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m input_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs:\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid Input name (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) for SignatureDef\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    247\u001b[0m                      input_name)\n\u001b[0;32m    248\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpreter_wrapper\u001b[38;5;241m.\u001b[39mResizeInputTensor(\n\u001b[0;32m    249\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs[input_name], np\u001b[38;5;241m.\u001b[39marray(value\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32),\n\u001b[0;32m    250\u001b[0m       \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subgraph_index)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Allocate tensors.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid Input name (x) for SignatureDef"
     ]
    }
   ],
   "source": [
    "output = my_signature(x=tf.constant(np.zeros(dim_input_data), shape=dim_input_data, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef037b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95b174f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmy_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: __call__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "my_signature(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# my_signature is callable with input as arguments.\n",
    "output = my_signature(x=tf.constant([1.0], shape=(1,10), dtype=tf.float32))\n",
    "# 'output' is dictionary with all outputs from the inference.\n",
    "# In this case we have single output 'result'.\n",
    "print(output['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f698cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
    "# This little helper wraps the TFLite Interpreter as a numpy-to-numpy function.\n",
    "def lite_model(images):\n",
    "    interpreter.allocate_tensors()\n",
    "    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
    "    interpreter.invoke()\n",
    "    return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "  probs_lite = lite_model(image[None, ...])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d85ec39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1ce34ebe100>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b087547",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e64041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "success, image = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c917ec69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[133, 152, 167],\n",
       "        [134, 153, 168],\n",
       "        [134, 153, 168],\n",
       "        ...,\n",
       "        [ 85, 103, 114],\n",
       "        [ 85, 103, 114],\n",
       "        [ 85, 103, 114]],\n",
       "\n",
       "       [[133, 152, 167],\n",
       "        [134, 153, 168],\n",
       "        [134, 153, 168],\n",
       "        ...,\n",
       "        [ 85, 103, 114],\n",
       "        [ 85, 103, 114],\n",
       "        [ 85, 103, 114]],\n",
       "\n",
       "       [[133, 152, 167],\n",
       "        [134, 153, 168],\n",
       "        [134, 153, 168],\n",
       "        ...,\n",
       "        [ 85, 103, 114],\n",
       "        [ 85, 103, 114],\n",
       "        [ 85, 103, 114]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[129, 149, 166],\n",
       "        [129, 149, 166],\n",
       "        [129, 149, 166],\n",
       "        ...,\n",
       "        [197, 217, 234],\n",
       "        [200, 220, 237],\n",
       "        [203, 223, 240]],\n",
       "\n",
       "       [[129, 149, 166],\n",
       "        [129, 149, 166],\n",
       "        [129, 149, 166],\n",
       "        ...,\n",
       "        [194, 214, 231],\n",
       "        [196, 216, 233],\n",
       "        [199, 219, 236]],\n",
       "\n",
       "       [[129, 149, 166],\n",
       "        [129, 149, 166],\n",
       "        [129, 149, 166],\n",
       "        ...,\n",
       "        [191, 211, 228],\n",
       "        [194, 214, 231],\n",
       "        [196, 216, 233]]], dtype=uint8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5701daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimensions :  (480, 640, 3)\n",
      "Resized Dimensions :  (384, 384, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Original Dimensions : ',image.shape)\n",
    " \n",
    "width = model.input_shape[1]\n",
    "height = model.input_shape[2] # keep original height\n",
    "dim = (width, height)\n",
    "  \n",
    "# resize image\n",
    "resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    " \n",
    "print('Resized Dimensions : ',resized.shape)\n",
    " \n",
    "cv2.imshow(\"Resized image\", resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3ffda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names =[\"Opaque\", \"Red\", \"Green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8375f783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: Red\n"
     ]
    }
   ],
   "source": [
    "width = model.input_shape[1]\n",
    "height = model.input_shape[2] # keep original height\n",
    "dim = (width, height)\n",
    "  \n",
    "# resize image\n",
    "resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "prediction_scores = model.predict(np.expand_dims(resized, axis=0))\n",
    "predicted_index = np.argmax(prediction_scores)\n",
    "print(\"Predicted label: \" + class_names[predicted_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f4bbcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "frames_to_rolling_mean = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d50743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predicted = class_names[predicted_index]\n",
    "predictions.append(prediction_scores)\n",
    "rolling_mean_prediction = np.mean(predictions[-frames_to_rolling_mean:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ac9be92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.297888, 152.98824 ,  72.04376 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_mean_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c0d2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_index = np.argmax(rolling_mean_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12873f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a99530d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predicted = class_names[predicted_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb15822a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.297888, 152.98824 ,  72.04376 ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_mean_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bf0dad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m max_pred \u001b[38;5;241m=\u001b[39m \u001b[43mrolling_mean_prediction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredicted_index\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "max_pred = rolling_mean_prediction[predicted_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffb50dea",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m predicted_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(rolling_mean_prediction)\n\u001b[0;32m      5\u001b[0m class_predicted \u001b[38;5;241m=\u001b[39m class_names[predicted_index]\n\u001b[1;32m----> 6\u001b[0m max_pred \u001b[38;5;241m=\u001b[39m \u001b[43mrolling_mean_prediction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredicted_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(max_pred)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "\n",
    "print(max_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68659b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db30fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81238567",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.append([0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75510f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " [0, 0, 0]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9842626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.append(prediction_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f332087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " [0, 0, 0]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7847e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\anaconda3\\envs\\signem\\lib\\site-packages\\numpy\\core\\_methods.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  6.74149323, 122.6668335 ,  57.49817505]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predictions[-5:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69fae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8f5d888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ac4c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_hands.Hands(\n",
    "        model_complexity=1,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image = np.zeros(image.shape)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6061a259",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMediaPipe Hands\u001b[39m\u001b[38;5;124m'\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mflip(image, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191937a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'continue' not properly in loop (3787763078.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [5]\u001b[1;36m\u001b[0m\n\u001b[1;33m    continue\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'continue' not properly in loop\n"
     ]
    }
   ],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "        model_complexity=1,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        # If loading a video, use 'break' instead of 'continue'.\n",
    "        continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image = np.zeros(image.shape)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cfa6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "        model_complexity=1,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            continue\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        image = np.zeros(image.shape)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        \n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fefcb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18143d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3418f2dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp_hands\u001b[38;5;241m.\u001b[39mHands(model_complexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      4\u001b[0m                     min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m      5\u001b[0m                     min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hands:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m----> 7\u001b[0m         success, image \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[0;32m      9\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIgnoring empty camera frame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(model_complexity=0,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            continue\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                                    image,\n",
    "                                    hand_landmarks,\n",
    "                                    mp_hands.HAND_CONNECTIONS,\n",
    "                                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                                    mp_drawing_styles.get_default_hand_connections_style())\n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f45a19a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Hands' object has no attribute 'HAND_CONNECTIONS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hand_landmarks \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[0;32m     18\u001b[0m         mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(\n\u001b[0;32m     19\u001b[0m             frame,\n\u001b[0;32m     20\u001b[0m             hand_landmarks,\n\u001b[1;32m---> 21\u001b[0m             \u001b[43mmp_hands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHAND_CONNECTIONS\u001b[49m,\n\u001b[0;32m     22\u001b[0m             mp_drawing_styles\u001b[38;5;241m.\u001b[39mget_default_hand_landmarks_style(),\n\u001b[0;32m     23\u001b[0m             mp_drawing_styles\u001b[38;5;241m.\u001b[39mget_default_hand_connections_style())\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Show the final output\u001b[39;00m\n\u001b[0;32m     26\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mflip(frame, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Hands' object has no attribute 'HAND_CONNECTIONS'"
     ]
    }
   ],
   "source": [
    "# Initialize the webcam for Hand Gesture Recognition Python project\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read each frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "    x , y, c = frame.shape\n",
    "\n",
    "    # Flip the frame vertically\n",
    "    #frame = cv2.flip(frame, 1)\n",
    "    #frame.flags.writeable = False\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame)\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", cv2.flip(frame, 1))\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "124ef974",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_videos = \"\"\n",
    "path_model = \"trained_models/signs_model_efficientnetv2-s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28940417",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sings = tf.saved_model.load(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79cbe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_videos = os.path.abspath(path_videos)\n",
    "\n",
    "os.chdir(path_videos)\n",
    "gestures = os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a5cf116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam for Hand Gesture Recognition Python project\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read each frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "    x , y, c = frame.shape\n",
    "\n",
    "    # Flip the frame vertically\n",
    "    #frame = cv2.flip(frame, 1)\n",
    "    frame = hs.handsegment(frame)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc5758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
