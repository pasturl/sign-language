{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ba4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436d594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('signs_model_efficientnetv2-s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d85ec39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1ce34ebe100>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b087547",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e64041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "success, image = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5701daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimensions :  (480, 640, 3)\n",
      "Resized Dimensions :  (384, 384, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Original Dimensions : ',image.shape)\n",
    " \n",
    "width = model.input_shape[1]\n",
    "height = model.input_shape[2] # keep original height\n",
    "dim = (width, height)\n",
    "  \n",
    "# resize image\n",
    "resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    " \n",
    "print('Resized Dimensions : ',resized.shape)\n",
    " \n",
    "cv2.imshow(\"Resized image\", resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3ffda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names =[\"Opaque\", \"Red\", \"Green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8375f783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: Red\n"
     ]
    }
   ],
   "source": [
    "width = model.input_shape[1]\n",
    "height = model.input_shape[2] # keep original height\n",
    "dim = (width, height)\n",
    "  \n",
    "# resize image\n",
    "resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "prediction_scores = model.predict(np.expand_dims(resized, axis=0))\n",
    "predicted_index = np.argmax(prediction_scores)\n",
    "print(\"Predicted label: \" + class_names[predicted_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f4bbcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "frames_to_rolling_mean = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d50743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predicted = class_names[predicted_index]\n",
    "predictions.append(prediction_scores)\n",
    "rolling_mean_prediction = np.mean(predictions[-frames_to_rolling_mean:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ac9be92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.297888, 152.98824 ,  72.04376 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_mean_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c0d2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_index = np.argmax(rolling_mean_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12873f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a99530d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predicted = class_names[predicted_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb15822a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.297888, 152.98824 ,  72.04376 ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_mean_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bf0dad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m max_pred \u001b[38;5;241m=\u001b[39m \u001b[43mrolling_mean_prediction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredicted_index\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "max_pred = rolling_mean_prediction[predicted_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffb50dea",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m predicted_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(rolling_mean_prediction)\n\u001b[0;32m      5\u001b[0m class_predicted \u001b[38;5;241m=\u001b[39m class_names[predicted_index]\n\u001b[1;32m----> 6\u001b[0m max_pred \u001b[38;5;241m=\u001b[39m \u001b[43mrolling_mean_prediction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredicted_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(max_pred)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "\n",
    "print(max_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68659b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db30fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81238567",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.append([0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75510f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " [0, 0, 0]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9842626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.append(prediction_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f332087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32),\n",
       " [0, 0, 0]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7847e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\anaconda3\\envs\\signem\\lib\\site-packages\\numpy\\core\\_methods.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  6.74149323, 122.6668335 ,  57.49817505]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predictions[-5:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69fae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8f5d888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.426867, 153.33354 ,  71.87272 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ac4c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_hands.Hands(\n",
    "        model_complexity=1,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image = np.zeros(image.shape)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6061a259",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMediaPipe Hands\u001b[39m\u001b[38;5;124m'\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mflip(image, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191937a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'continue' not properly in loop (3787763078.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [5]\u001b[1;36m\u001b[0m\n\u001b[1;33m    continue\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'continue' not properly in loop\n"
     ]
    }
   ],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "        model_complexity=1,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        # If loading a video, use 'break' instead of 'continue'.\n",
    "        continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image = np.zeros(image.shape)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cfa6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "        model_complexity=1,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            continue\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        image = np.zeros(image.shape)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        \n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fefcb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18143d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3418f2dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp_hands\u001b[38;5;241m.\u001b[39mHands(model_complexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      4\u001b[0m                     min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m      5\u001b[0m                     min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hands:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m----> 7\u001b[0m         success, image \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[0;32m      9\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIgnoring empty camera frame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(model_complexity=0,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            continue\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                                    image,\n",
    "                                    hand_landmarks,\n",
    "                                    mp_hands.HAND_CONNECTIONS,\n",
    "                                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                                    mp_drawing_styles.get_default_hand_connections_style())\n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f45a19a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Hands' object has no attribute 'HAND_CONNECTIONS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hand_landmarks \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[0;32m     18\u001b[0m         mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(\n\u001b[0;32m     19\u001b[0m             frame,\n\u001b[0;32m     20\u001b[0m             hand_landmarks,\n\u001b[1;32m---> 21\u001b[0m             \u001b[43mmp_hands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHAND_CONNECTIONS\u001b[49m,\n\u001b[0;32m     22\u001b[0m             mp_drawing_styles\u001b[38;5;241m.\u001b[39mget_default_hand_landmarks_style(),\n\u001b[0;32m     23\u001b[0m             mp_drawing_styles\u001b[38;5;241m.\u001b[39mget_default_hand_connections_style())\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Show the final output\u001b[39;00m\n\u001b[0;32m     26\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mflip(frame, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Hands' object has no attribute 'HAND_CONNECTIONS'"
     ]
    }
   ],
   "source": [
    "# Initialize the webcam for Hand Gesture Recognition Python project\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read each frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "    x , y, c = frame.shape\n",
    "\n",
    "    # Flip the frame vertically\n",
    "    #frame = cv2.flip(frame, 1)\n",
    "    #frame.flags.writeable = False\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame)\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", cv2.flip(frame, 1))\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "124ef974",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_videos = \"\"\n",
    "path_model = \"trained_models/signs_model_efficientnetv2-s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28940417",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sings = tf.saved_model.load(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79cbe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_videos = os.path.abspath(path_videos)\n",
    "\n",
    "os.chdir(path_videos)\n",
    "gestures = os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a5cf116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam for Hand Gesture Recognition Python project\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read each frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "    x , y, c = frame.shape\n",
    "\n",
    "    # Flip the frame vertically\n",
    "    #frame = cv2.flip(frame, 1)\n",
    "    frame = hs.handsegment(frame)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Show the final output\n",
    "    cv2.imshow(\"Output\", frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc5758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
